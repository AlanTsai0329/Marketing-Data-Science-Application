{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# 透過機器學習預測股市漲跌-基本資料處理\r\n",
                "## 作者：蔡尚宏(臺灣行銷研究特邀作者)、劉睿哲(臺灣行銷研究特邀作者)、鄭晴文(臺灣行銷研究特邀作者)\r\n",
                "## 原始資料請見[本連結](https://www.kaggle.com/aaron7sun/stocknews)，下載下來後將pd.read_csv()內的路徑改為資料集的路徑即可"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\r\n",
                "import re\r\n",
                "import nltk\r\n",
                "import string\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "from nltk.corpus import stopwords\r\n",
                "from nltk.tokenize import word_tokenize\r\n",
                "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
                "from sklearn.linear_model import LogisticRegression\r\n",
                "from sklearn.model_selection import train_test_split\r\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\r\n",
                "from datetime import timedelta, datetime\r\n",
                "#from google.colab import drive\r\n",
                "#drive.mount('/content/drive')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "nltk.download('punkt')\r\n",
                "nltk.download('stopwords')\r\n",
                "nltk.download('wordnet')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\vivia\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\vivia\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\vivia\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 2
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "data = pd.read_csv(r'C:\\Users\\vivia\\Downloads\\archive\\Combined_News_DJIA.csv')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "def preprocess(processdata):\r\n",
                "    # 將所有字母轉為小寫\r\n",
                "    headlines = []\r\n",
                "    for i in range(1, 21):\r\n",
                "      headlines.append('Top'+str(i))\r\n",
                "    processdata[headlines] = processdata[headlines].astype(str)\r\n",
                "    processdata[headlines] = processdata[headlines].applymap(str.lower)\r\n",
                "    \r\n",
                "    # 組成以天為單位的data\r\n",
                "    processdata_headlines = []\r\n",
                "    for row in range(0,len(processdata.index)):\r\n",
                "      processdata_headlines.append(' '.join(str(x) for x in processdata.iloc[row,2:27]))\r\n",
                "\r\n",
                "    # 刪除數字與標點符號\r\n",
                "    for line in range(len(processdata_headlines)):\r\n",
                "      processdata_headlines[line] = re.sub(r'[^A-Za-z]',\" \", processdata_headlines[line])\r\n",
                "\r\n",
                "    # 將標題切為個別字詞\r\n",
                "    for sentence in range(len(processdata_headlines)):\r\n",
                "      processdata_headlines[sentence] = word_tokenize(processdata_headlines[sentence]) \r\n",
                "\r\n",
                "    # 去除英文字母及停用詞\r\n",
                "    alpha = []\r\n",
                "    for abc in string.ascii_lowercase :\r\n",
                "      alpha.append(abc)      \r\n",
                "    en_stops = stopwords.words('english')\r\n",
                "    en_stops.extend(alpha)\r\n",
                "    for sentence in range(len(processdata_headlines)):\r\n",
                "      processdata_headlines[sentence] = [w for w in processdata_headlines[sentence] if w not in en_stops] \r\n",
                "    \r\n",
                "    # 詞幹提取與詞形還原\r\n",
                "    for sentence in range(len(processdata_headlines)):\r\n",
                "      processdata_headlines[sentence] = [WordNetLemmatizer().lemmatize(w) for w in processdata_headlines[sentence]]\r\n",
                "      processdata_headlines[sentence] = [WordNetLemmatizer().lemmatize(w, pos='v') for w in processdata_headlines[sentence]]   \r\n",
                "\r\n",
                "    # 組回標題\r\n",
                "    final_processdata_headlines = []\r\n",
                "    for words in processdata_headlines :\r\n",
                "      filter_words = \"\"\r\n",
                "      for i in range(len(words)) :\r\n",
                "        filter_words = filter_words + words[i] + \" \"\r\n",
                "      final_processdata_headlines.append(filter_words)  \r\n",
                "\r\n",
                "    return final_processdata_headlines"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 資料按照時間切"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "train = data[data['Date'] < '2015-01-01']\r\n",
                "test = data[data['Date'] > '2014-12-31']\r\n",
                "final_traindata = preprocess(train)\r\n",
                "final_testdata = preprocess(test)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\vivia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3607: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  self._set_item(key, value)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### 資料處理完的結果"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# 此為第一天25則標題處理完，並串聯而成的句子\r\n",
                "print(final_traindata[0])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "georgia down two russian warplane country move brink war break musharraf impeach russia today column troop roll south ossetia footage fight youtube russian tank move towards capital south ossetia reportedly completely destroy georgian artillery fire afghan child rap impunity official say sick three year old rap nothing russian tank enter south ossetia whilst georgia shoot two russian jet break georgia invade south ossetia russia warn would intervene side enemy combatent trial nothing sham salim haman sentence year keep longer anyway feel like georgian troop retreat osettain capital presumably leave several hundred people kill video prep georgia war russia rice give green light israel attack iran say veto israeli military ops announce class action lawsuit behalf american public fbi russia georgia war nyt top story open ceremony olympics fuck disgrace yet proof decline journalism china tell bush stay country affair world war iii start today georgia invade south ossetia russia get involve nato absorb georgia unleash full scale war al qaeda face islamist backlash condoleezza rice u would act prevent israeli strike iran israeli defense minister ehud barak israel prepare uncompromising victory case military hostility busy day european union approve new sanction iran protest nuclear programme georgia withdraw soldier iraq help fight russian force georgia breakaway region south ossetia Why Pentagon Thinks Attacking Iran Bad Idea US News amp World Report Caucasus crisis Georgia invade South Ossetia Indian shoe manufactory And series like work Visitors Suffering Mental Illnesses Banned Olympics No Help Mexico Kidnapping Surge \n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.4 64-bit"
        },
        "interpreter": {
            "hash": "7ac4e6d1a5b9325e6852d40eac93029866276e599590016c337198456c87e9d8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}