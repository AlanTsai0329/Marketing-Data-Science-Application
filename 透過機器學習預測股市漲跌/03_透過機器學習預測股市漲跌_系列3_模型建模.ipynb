{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Kaggle side project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.2 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "5d87b42201e74ac320bc00dce267d44f5f134edfec9046f67f672f289707ff6a"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "# 透過機器學習預測股市漲跌-模型建模\r\n",
        "## 作者：蔡尚宏（臺灣行銷研究特邀作者）、劉睿哲（臺灣行銷研究特邀作者）、鄭晴文（臺灣行銷研究特邀作者）、鍾皓軒(臺灣行銷研究有限公司創辦人）\r\n",
        "## 縮寫還原完的資料請見[本連結](https://drive.google.com/file/d/1HfZvdy0nJYbPN_tB9cBAotdAeGWkm9Re/view?usp=sharing)，下載下來後與本ipynb檔案放於同一個工作目錄中，再執行下方程式即可"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "import string\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from datetime import timedelta, datetime\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "2IEhgxU30oqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgwvubK02UEc",
        "outputId": "80d4f80b-d9de-441b-825c-8ce0c3e468b5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "data = pd.read_csv(\"after_Combined_News_DJIA.csv\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "AugePrAM1jzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 系列一_透過機器學習預測股市漲跌-基本資料處理"
      ],
      "metadata": {
        "id": "XcX33McdPwsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def preprocess(processdata):\r\n",
        "    # 轉小寫\r\n",
        "    headlines = []\r\n",
        "    for i in range(1, 26):\r\n",
        "      headlines.append('Top'+str(i))\r\n",
        "    processdata[headlines] = processdata[headlines].astype(str)\r\n",
        "    processdata[headlines] = processdata[headlines].applymap(str.lower)\r\n",
        "    \r\n",
        "    # 組成以天為單位的data\r\n",
        "    processdata_headlines = []\r\n",
        "    for row in range(0,len(processdata.index)):\r\n",
        "      processdata_headlines.append(' '.join(str(x) for x in processdata.iloc[row,2:27]))\r\n",
        "\r\n",
        "    # remove punctuation characters\r\n",
        "    for line in range(len(processdata_headlines)):\r\n",
        "      processdata_headlines[line] = re.sub(r'[^A-Za-z]',\" \", processdata_headlines[line])\r\n",
        "\r\n",
        "    # 切字\r\n",
        "    for sentence in range(len(processdata_headlines)):\r\n",
        "      processdata_headlines[sentence] = word_tokenize(processdata_headlines[sentence]) \r\n",
        "\r\n",
        "    # 去除停用詞\r\n",
        "    alpha = []\r\n",
        "    for abc in string.ascii_lowercase :\r\n",
        "      alpha.append(abc)      \r\n",
        "    en_stops = stopwords.words('english')\r\n",
        "    en_stops.extend(alpha)\r\n",
        "    for sentence in range(len(processdata_headlines)):\r\n",
        "      processdata_headlines[sentence] = [w for w in processdata_headlines[sentence] if w not in en_stops] \r\n",
        "    \r\n",
        "    # 單字變回原形\r\n",
        "    for sentence in range(len(processdata_headlines)):\r\n",
        "      processdata_headlines[sentence] = [WordNetLemmatizer().lemmatize(w) for w in processdata_headlines[sentence]]\r\n",
        "      processdata_headlines[sentence] = [WordNetLemmatizer().lemmatize(w, pos='v') for w in processdata_headlines[sentence]]   \r\n",
        "\r\n",
        "    # 組回標題\r\n",
        "    final_processdata_headlines = []\r\n",
        "    for words in processdata_headlines :\r\n",
        "      filter_words = \"\"\r\n",
        "      for i in range(len(words)) :\r\n",
        "        filter_words = filter_words + words[i] + \" \"\r\n",
        "      final_processdata_headlines.append(filter_words)  \r\n",
        "\r\n",
        "    return final_processdata_headlines  "
      ],
      "outputs": [],
      "metadata": {
        "id": "S8A0Xz54F9Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 依照時間切分訓練集、測試集"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "train = data[data['Date'] < '2015-01-01']\r\n",
        "test = data[data['Date'] > '2014-12-31']\r\n",
        "final_traindata = preprocess(train)\r\n",
        "final_testdata = preprocess(test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py:3636: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 系列二_透過機器學習預測股市漲跌-進階資料處理"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "tfidf_vector = TfidfVectorizer(min_df=0.01, max_df=0.99, max_features=160, ngram_range=(2, 2))\r\n",
        "final_traindata_tfidf = tfidf_vector.fit_transform(final_traindata)\r\n",
        "final_testdata_tfidf = tfidf_vector.transform(final_testdata)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "word = tfidf_vector.get_feature_names()\r\n",
        "df = pd.DataFrame(final_traindata_tfidf.T.todense().transpose(), columns=word).sum(axis=0)\r\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "air force     15.463068\n",
              "air strike    22.806328\n",
              "al jazeera    37.810893\n",
              "al qaeda      40.214684\n",
              "al qaida      19.244321\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 系列三_透過機器學習預測股市漲跌-模型建模"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 查看模型輸入"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "word = tfidf_vector.get_feature_names()\r\n",
        "input_df = pd.DataFrame(final_traindata_tfidf.todense(), columns=word)\r\n",
        "input_df.head(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>air force</th>\n",
              "      <th>air strike</th>\n",
              "      <th>al jazeera</th>\n",
              "      <th>al qaeda</th>\n",
              "      <th>al qaida</th>\n",
              "      <th>anti gay</th>\n",
              "      <th>around world</th>\n",
              "      <th>australian government</th>\n",
              "      <th>barack obama</th>\n",
              "      <th>bbc news</th>\n",
              "      <th>...</th>\n",
              "      <th>world biggest</th>\n",
              "      <th>world cup</th>\n",
              "      <th>world first</th>\n",
              "      <th>world largest</th>\n",
              "      <th>world news</th>\n",
              "      <th>world war</th>\n",
              "      <th>year ago</th>\n",
              "      <th>year jail</th>\n",
              "      <th>year old</th>\n",
              "      <th>year prison</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.361791</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.228241</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.671348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.790220</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.337538</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.375588</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.298680</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.606469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.391499</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.233103</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.375292</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 160 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   air force  air strike  al jazeera  al qaeda  al qaida  anti gay  \\\n",
              "0        0.0         0.0         0.0  0.361791       0.0       0.0   \n",
              "1        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "2        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "3        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "4        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "5        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "6        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "7        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "8        0.0         0.0         0.0  0.375292       0.0       0.0   \n",
              "9        0.0         0.0         0.0  0.000000       0.0       0.0   \n",
              "\n",
              "   around world  australian government  barack obama  bbc news  ...  \\\n",
              "0      0.000000                    0.0           0.0       0.0  ...   \n",
              "1      0.000000                    0.0           0.0       0.0  ...   \n",
              "2      0.000000                    0.0           0.0       0.0  ...   \n",
              "3      0.000000                    0.0           0.0       0.0  ...   \n",
              "4      0.000000                    0.0           0.0       0.0  ...   \n",
              "5      0.000000                    0.0           0.0       0.0  ...   \n",
              "6      0.606469                    0.0           0.0       0.0  ...   \n",
              "7      0.000000                    0.0           0.0       0.0  ...   \n",
              "8      0.000000                    0.0           0.0       0.0  ...   \n",
              "9      0.000000                    0.0           0.0       0.0  ...   \n",
              "\n",
              "   world biggest  world cup  world first  world largest  world news  \\\n",
              "0            0.0        0.0          0.0            0.0    0.000000   \n",
              "1            0.0        0.0          0.0            0.0    0.000000   \n",
              "2            0.0        0.0          0.0            0.0    0.000000   \n",
              "3            0.0        0.0          0.0            0.0    0.000000   \n",
              "4            0.0        0.0          0.0            0.0    0.375588   \n",
              "5            0.0        0.0          0.0            0.0    0.000000   \n",
              "6            0.0        0.0          0.0            0.0    0.000000   \n",
              "7            0.0        0.0          0.0            0.0    0.000000   \n",
              "8            0.0        0.0          0.0            0.0    0.000000   \n",
              "9            0.0        0.0          0.0            0.0    0.000000   \n",
              "\n",
              "   world war  year ago  year jail  year old  year prison  \n",
              "0   0.400727  0.000000        0.0  0.228241          0.0  \n",
              "1   0.671348  0.000000        0.0  0.000000          0.0  \n",
              "2   0.000000  0.000000        0.0  0.790220          0.0  \n",
              "3   0.000000  0.000000        0.0  0.337538          0.0  \n",
              "4   0.000000  0.000000        0.0  0.000000          0.0  \n",
              "5   0.000000  0.000000        0.0  0.298680          0.0  \n",
              "6   0.000000  0.000000        0.0  0.000000          0.0  \n",
              "7   0.000000  0.391499        0.0  0.233103          0.0  \n",
              "8   0.000000  0.000000        0.0  0.000000          0.0  \n",
              "9   0.000000  0.000000        0.0  0.000000          0.0  \n",
              "\n",
              "[10 rows x 160 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression 模型"
      ],
      "metadata": {
        "id": "xW1MXG4v7sY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "lr_random_grid ={'solver' : [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"], \r\n",
        "                   'penalty' : [\"l1\", \"l2\"], \r\n",
        "                   \"C\" : [x for x in np.arange(0.0001, 1000, 10)] ,  \r\n",
        "                   \"max_iter\" : [int(x) for x in range(1,500,10)],\r\n",
        "                   \"class_weight\" : ['balanced']}\r\n",
        "\r\n",
        "lr_random_model = LogisticRegression()\r\n",
        "lr_random_search = RandomizedSearchCV(estimator=lr_random_model, param_distributions=lr_random_grid, n_iter = 100, \r\n",
        "                                        scoring='accuracy', cv = 3, verbose=2, random_state=42, n_jobs=-1)\r\n",
        "\r\n",
        "# Fit the random search model\r\n",
        "lr_random_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "random_lr_model = lr_random_search.best_estimator_\r\n",
        "predictions = random_lr_model.predict(final_testdata_tfidf)\r\n",
        "\r\n",
        "print(\"Score of train set: % .10f\" % (random_lr_model.score(final_traindata_tfidf, train[\"Label\"])))\r\n",
        "print(\"Score of test set: % .10f\" % (random_lr_model.score(final_testdata_tfidf, test[\"Label\"])))\r\n",
        "print(\"Best score:{}\".format(lr_random_search.best_score_))\r\n",
        "print(\"Best parameters:{}\".format(lr_random_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "lr_grid ={'solver' : [\"newton-cg\", \"liblinear\", \"sag\"], \r\n",
        "          'penalty' : [\"l2\"], \r\n",
        "          \"C\" : [x for x in np.arange(100, 200, 10)] ,  \r\n",
        "          \"max_iter\" : [int(x) for x in range(200, 400, 20)],\r\n",
        "          \"class_weight\" : ['balanced']}\r\n",
        "\r\n",
        "lr_model = LogisticRegression()\r\n",
        "lr_grid_search = GridSearchCV(lr_model, lr_grid, scoring='accuracy')\r\n",
        "lr_grid_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "grid_lr_model = lr_grid_search.best_estimator_\r\n",
        "print(\"Score of train set: % .10f\" % (grid_lr_model.score(final_traindata_tfidf, train[\"Label\"])))\r\n",
        "print(\"Score of test set: % .10f\" % (grid_lr_model.score(final_testdata_tfidf, test[\"Label\"])))\r\n",
        "print(\"Best score:{}\".format(lr_grid_search.best_score_))\r\n",
        "print(\"Best parameters:{}\".format(lr_grid_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 最終結果"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "lr_model = LogisticRegression()\r\n",
        "lr_model.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "train_pred = lr_model.predict(final_traindata_tfidf)\r\n",
        "test_pred = lr_model.predict(final_testdata_tfidf)\r\n",
        "\r\n",
        "train_accuracy = accuracy_score(train['Label'], train_pred)\r\n",
        "test_accuracy = accuracy_score(test['Label'], test_pred)\r\n",
        "print(\"Accuracy of train set ：{:.4f}\".format(train_accuracy))\r\n",
        "print(\"Accuracy of test set：{:.4f}\".format(test_accuracy))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of train set ：0.6394\n",
            "Accuracy of test set：0.5423\n"
          ]
        }
      ],
      "metadata": {
        "id": "IglYA0v_8-wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest 模型"
      ],
      "metadata": {
        "id": "qKTd8C0j9IBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "n_estimators = [int(x) for x in np.linspace(60, 160, num = 20)]\r\n",
        "max_features = ['auto', 'sqrt'] \r\n",
        "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\r\n",
        "max_depth.append(None)\r\n",
        "min_samples_split = [2, 4, 5, 7, 8, 10]\r\n",
        "min_samples_leaf = [1, 2, 3, 4, 5, 6]\r\n",
        "bootstrap = [True, False]\r\n",
        "criterion = ['entropy']\r\n",
        "random_state = [0]\r\n",
        "\r\n",
        "rfc_random_grid = {'n_estimators': n_estimators,\r\n",
        "               'max_features': max_features,\r\n",
        "               'max_depth': max_depth,\r\n",
        "               'min_samples_split': min_samples_split,\r\n",
        "               'min_samples_leaf': min_samples_leaf,\r\n",
        "               'bootstrap': bootstrap,\r\n",
        "               'random_state':random_state,\r\n",
        "               'criterion':criterion}\r\n",
        "\r\n",
        "rfc = RandomForestClassifier()\r\n",
        "rfc_random_search = RandomizedSearchCV(estimator=rfc, param_distributions=rfc_random_grid, n_iter = 100, scoring='accuracy', \r\n",
        "                               cv = 3, verbose=2, random_state=42, n_jobs=-1)\r\n",
        "rfc_random_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "rfc_random_model = rfc_random_search.best_estimator_\r\n",
        "print(\"Score of train set: % .10f\" % (rfc_random_model.score(final_traindata_tfidf, train[\"Label\"])))\r\n",
        "print(\"Score of test set: % .10f\" % (rfc_random_model.score(final_testdata_tfidf, test[\"Label\"])))\r\n",
        "print(\"Best score:{}\".format(rfc_random_search.best_score_))\r\n",
        "print(\"Best parameters:{}\".format(rfc_random_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "n_estimators = [130,138] \r\n",
        "max_features = ['auto'] \r\n",
        "max_depth = [5,8,10]\r\n",
        "max_depth.append(None)\r\n",
        "min_samples_split = [2,3]\r\n",
        "min_samples_leaf = [None,2,6]\r\n",
        "bootstrap = [True]\r\n",
        "criterion = ['entropy']\r\n",
        "random_state = [0]\r\n",
        "\r\n",
        "rfc_param_grid = {\"random_state\":random_state,\r\n",
        "                  \"max_features\":max_features,\r\n",
        "                  \"n_estimators\":n_estimators,\r\n",
        "                  \"max_depth\":max_depth,\r\n",
        "                  \"min_samples_leaf\":min_samples_leaf,\r\n",
        "                  \"min_samples_split\":min_samples_split,\r\n",
        "                  \"criterion\":criterion}\r\n",
        "\r\n",
        "rfc = RandomForestClassifier()\r\n",
        "rfc_grid_search = GridSearchCV(rfc, rfc_param_grid, scoring='accuracy')\r\n",
        "rfc_grid_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "rfc_grid_model = rfc_grid_search.best_estimator_\r\n",
        "print(\"Score of train set: % .10f\" % (rfc_grid_model.score(final_traindata_tfidf, train[\"Label\"])))\r\n",
        "print(\"Score of test set: % .10f\" % (rfc_grid_model.score(final_testdata_tfidf, test[\"Label\"])))\r\n",
        "print(\"Best score:{}\".format(rfc_grid_search.best_score_))\r\n",
        "print(\"Best parameters:{}\".format(rfc_grid_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 最終結果"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "rfc = RandomForestClassifier(n_estimators = 138 ,criterion = 'gini' ,min_samples_split = 7, max_depth = 10, random_state=0)\r\n",
        "rfc.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "train_pred = rfc.predict(final_traindata_tfidf)\r\n",
        "test_pred = rfc.predict(final_testdata_tfidf)\r\n",
        "\r\n",
        "train_accuracy = accuracy_score(train['Label'], train_pred)\r\n",
        "test_accuracy = accuracy_score(test['Label'], test_pred)\r\n",
        "print(\"Accuracy of train set ：{:.4f}\".format(train_accuracy))\r\n",
        "print(\"Accuracy of test set：{:.4f}\".format(test_accuracy))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of train set ：0.7511\n",
            "Accuracy of test set：0.5423\n"
          ]
        }
      ],
      "metadata": {
        "id": "aRJCc2ZD9OHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes 模型"
      ],
      "metadata": {
        "id": "8bxknaWx9jGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "nb_random_grid = {'alpha': [x for x in np.arange(0.001,100,0.01)]}\r\n",
        "\r\n",
        "nb_model = MultinomialNB()\r\n",
        "nb_random_search = RandomizedSearchCV(estimator=nb_model, param_distributions=nb_random_grid,\r\n",
        "                              n_iter = 100, scoring='accuracy', \r\n",
        "                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\r\n",
        "\r\n",
        "nb_random_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "nb_result = nb_random_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "nb_random_model = nb_random_search.best_estimator_\r\n",
        "\r\n",
        "print(\"Scroe of train set: % .10f\" % (nb_random_model.score(final_traindata_tfidf, train[\"Label\"])))\r\n",
        "print(\"Scroe of test set: % .10f\" % (nb_random_model.score(final_testdata_tfidf, test[\"Label\"])))\r\n",
        "print(\"Best score:{}\".format(nb_random_search.best_score_))\r\n",
        "print(\"Best parameters:{}\".format(nb_random_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# 調參\r\n",
        "nb_param_grid= {'alpha': [x for x in np.arange(0.1, 80, 0.1)]}\r\n",
        "nb_model = MultinomialNB()\r\n",
        "nb_grid_search = GridSearchCV(nb_model, nb_param_grid, scoring='accuracy', cv=5)\r\n",
        "nb_result = nb_grid_search.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "nb_grid_model = nb_grid_search.best_estimator_\r\n",
        "\r\n",
        "print(\"Scroe of train set: % .10f\" % (nb_grid_model.score(final_traindata_tfidf, train[\"Label\"])))\r\n",
        "print(\"Scroe of test set: % .10f\" % (nb_grid_model.score(final_testdata_tfidf, test[\"Label\"])))\r\n",
        "print(\"Best score:{}\".format(nb_grid_search.best_score_))\r\n",
        "print(\"Best parameters:{}\".format(nb_grid_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 最終結果"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "nb_model = MultinomialNB(alpha=1.8)\r\n",
        "nb_model.fit(final_traindata_tfidf, train[\"Label\"])\r\n",
        "\r\n",
        "train_pred = nb_model.predict(final_traindata_tfidf)\r\n",
        "test_pred = nb_model.predict(final_testdata_tfidf)\r\n",
        "\r\n",
        "train_accuracy = accuracy_score(train['Label'], train_pred)\r\n",
        "test_accuracy = accuracy_score(test['Label'], test_pred)\r\n",
        "print(\"Accuracy of train set ：{:.4f}\".format(train_accuracy))\r\n",
        "print(\"Accuracy of test set：{:.4f}\".format(test_accuracy))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of train set ：0.6077\n",
            "Accuracy of test set：0.5317\n"
          ]
        }
      ],
      "metadata": {
        "id": "NIgv71Vr9nri"
      }
    }
  ]
}